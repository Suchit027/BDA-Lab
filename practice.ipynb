{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b6c775d-15df-4def-9468-265a0b39cd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['this is a test. this is a test', 'hello', 'another text file.']\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster('local[*]'))\n",
    "rdd = sc.textFile('./text file')\n",
    "print(rdd.getNumPartitions())\n",
    "print(rdd.collect())\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fdbe68c-91ae-4cda-81ed-a98191b94db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('file:/mnt/c/Users/Suchit/Desktop/5th Sem/Big Data/lab/text file/text.txt', 'this is a test. this is a test\\r\\nhello'), ('file:/mnt/c/Users/Suchit/Desktop/5th Sem/Big Data/lab/text file/text1.txt', 'another text file.')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster('local[*]'))\n",
    "rdd = sc.wholeTextFiles('./text file')\n",
    "print(rdd.collect())\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d87c7c9-f117-4d0b-832c-f0f50cdc6ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[5] at RDD at PythonRDD.scala:53\n",
      "[1, 3, 5, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster('local[*]'))\n",
    "rdd = sc.range(1, 10, 2, 1)\n",
    "print(rdd)\n",
    "print(rdd.collect())\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8200900c-c88d-407d-adaf-81685967d508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster('local[*]'))\n",
    "rdd = sc.parallelize([1,2,3,4])\n",
    "a = rdd.min()\n",
    "print(a)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f983d0ea-4241-4530-be70-a9cd1ad816f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('u', 1), ('i', 2), ('a', 1), ('e', 1)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster('local[*]'))\n",
    "rdd = sc.parallelize(['My name is Suchit']).flatMap(lambda x: x)\n",
    "r1 = rdd.map(lambda x: (x, 1))\n",
    "r2 = r1.filter(lambda x: x[0] in 'aeiouAEIOU')\n",
    "r3 = r2.reduceByKey(lambda x, y: x + y)\n",
    "print(r3.collect())\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e9dcdce-6ba9-4a49-af47-6bb006aa2658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('name', 1), ('My', 1), ('is', 1), ('Suchit', 1)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster('local[*]'))\n",
    "rdd = sc.parallelize(['My name is Suchit']).map(lambda x: x.split()).flatMap(lambda x: x)\n",
    "r1 = rdd.map(lambda x: (x, 1))\n",
    "r3 = r1.reduceByKey(lambda x, y: x + y)\n",
    "print(r3.collect())\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d5a983e-0b9c-483f-b3e4-9dc49230db1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('u', [1]), ('i', [1, 1]), ('a', [1]), ('e', [1])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster('local[*]'))\n",
    "rdd = sc.parallelize(['My name is Suchit']).flatMap(lambda x: x)\n",
    "r1 = rdd.map(lambda x: (x, 1))\n",
    "r2 = r1.filter(lambda x: x[0] in 'aeiouAEIOU')\n",
    "r3 = r2.groupByKey().mapValues(lambda x: list(x))\n",
    "print(r3.collect())\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94f1b801-42e7-4135-944f-f70cae122291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My name is Suchit', 'abcd']\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster('local[*]'))\n",
    "rdd = sc.parallelize([['My name is Suchit'], ['abcd']]).flatMap(lambda x: x)\n",
    "print(rdd.collect())\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59965e12-c511-4615-be74-e6dc254b97f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/13 18:38:11 WARN Utils: Your hostname, DESKTOP-2J74AJH resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/10/13 18:38:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/13 18:38:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no. of sentences = 3\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster('local[*]'))\n",
    "rdd = sc.textFile('test.txt')\n",
    "rdd = rdd.map(lambda x: list(x)).flatMap(lambda x: x)\n",
    "r1 = rdd.map(lambda x: (x, 1))\n",
    "r2 = r1.filter(lambda x: x[0] in '.?!')\n",
    "r3 = r2.reduceByKey(lambda x, y: x + y)\n",
    "r4 = r3.map(lambda x: x[1])\n",
    "r4 = r4.fold(0, lambda x, y: x + y)\n",
    "print(f' no. of sentences = {r4}')\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70451e-b07b-42c4-aa35-28cbc306d1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
